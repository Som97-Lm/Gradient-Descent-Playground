# Gradient Descent Playground

This repository contains a hands-on notebook that demonstrates how **Gradient Descent** works in machine learning.  
It includes experiments on loss functions, convergence curves, learning rate selection, and visual intuition behind the optimization process.

---

##  Notebook Included
- **Gradient_Descent.ipynb**

This notebook covers:
- Manual implementation of Gradient Descent  
- Understanding cost/loss functions  
- Calculating gradients step-by-step  
- Effects of different learning rates  
- Convergence and divergence examples  
- Plotting loss vs iterations  
- Intuition behind how optimization improves model performance  

---

##  Objective
To provide an intuitive and visual understanding of:
- How Gradient Descent optimizes ML models  
- Why learning rate matters  
- How convergence happens  
- How to avoid divergence or slow learning  

This notebook helps build a deep conceptual foundation for training linear models and neural networks.

---

##  Technologies Used
- Python  
- NumPy  
- Matplotlib  
- Plotly (for optional interactive plots)  

---

##  How to Use This Repository
1. Clone the repository:

git clone https://github.com/Som97-Lm/Gradient-Descent-Playground.git

2. Open the notebook in Jupyter:
3. Run the experiments and modify parameters such as learning rate and iteration count to see their impact.

---

##  Future Enhancements
- Add stochastic and mini-batch gradient descent  
- Add momentum and Adam optimizer demos  
- Add multiple loss function examples  
- Add 3D surface + contour plots for better visualization  

---

##  Author
**Soumen Manna**  
Machine Learning & Data Science Practitioner  
Focused on understanding models from first principles through hands-on experimentation.

---

If this notebook helps you, please **star the repository**!
